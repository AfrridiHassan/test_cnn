{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12720504,"sourceType":"datasetVersion","datasetId":8040081}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1 — Setup & Data Load (UPDATED)\nimport os, json\nimport pandas as pd\n\n# Kaggle path to DiverseVul JSONL\nDATA_PATH = \"/kaggle/input/diversevul-20230702/diversevul_20230702.json\"\n\n# Load entire JSONL into a DataFrame\nrows = []\nwith open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        rows.append(json.loads(line))\n\ndf = pd.DataFrame(rows)\nprint(\"raw df shape:\", df.shape)\nprint(\"columns:\", list(df.columns))\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T10:44:49.413873Z","iopub.execute_input":"2025-08-19T10:44:49.414099Z","iopub.status.idle":"2025-08-19T10:45:05.605656Z","shell.execute_reply.started":"2025-08-19T10:44:49.414076Z","shell.execute_reply":"2025-08-19T10:45:05.604927Z"}},"outputs":[{"name":"stdout","text":"raw df shape: (330492, 8)\ncolumns: ['func', 'target', 'cwe', 'project', 'commit_id', 'hash', 'size', 'message']\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"                                                func  target        cwe  \\\n0  int _gnutls_ciphertext2compressed(gnutls_sessi...       1         []   \n1  static char *make_filename_safe(const char *fi...       1  [CWE-264]   \n2  unpack_Z_stream(int fd_in, int fd_out)\\n{\\n\\tI...       1         []   \n3  static void cirrus_do_copy(CirrusVGAState *s, ...       1  [CWE-787]   \n4  glue(cirrus_bitblt_rop_fwd_, ROP_NAME)(CirrusV...       1  [CWE-787]   \n\n   project                                 commit_id  \\\n0   gnutls  7ad6162573ba79a4392c63b453ad0220ca6c5ace   \n1  php-src  055ecbc62878e86287d742c7246c21606cee8183   \n2  busybox  251fc70e9722f931eec23a34030d05ba5f747b0e   \n3     qemu  b2eb849d4b1fdb6f35d5c46958c7f703cf64cfef   \n4     qemu  b2eb849d4b1fdb6f35d5c46958c7f703cf64cfef   \n\n                                      hash  size  \\\n0   73008646937836648589283922871188272089   157   \n1  211824207069112513181516095447837228041    22   \n2   21401706257394042943815500829552774160   232   \n3  135590882627853658533498335902319684573    66   \n4   27696392987383562433164405181263025184    18   \n\n                                             message  \n0   added an extra check while checking the padding.  \n1  Improve check for :memory: pseudo-filename in ...  \n2  uncompress: fix buffer underrun by corrupted i...  \n3  CVE-2007-1320 - Cirrus LGD-54XX \"bitblt\" heap ...  \n4  CVE-2007-1320 - Cirrus LGD-54XX \"bitblt\" heap ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>func</th>\n      <th>target</th>\n      <th>cwe</th>\n      <th>project</th>\n      <th>commit_id</th>\n      <th>hash</th>\n      <th>size</th>\n      <th>message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>int _gnutls_ciphertext2compressed(gnutls_sessi...</td>\n      <td>1</td>\n      <td>[]</td>\n      <td>gnutls</td>\n      <td>7ad6162573ba79a4392c63b453ad0220ca6c5ace</td>\n      <td>73008646937836648589283922871188272089</td>\n      <td>157</td>\n      <td>added an extra check while checking the padding.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>static char *make_filename_safe(const char *fi...</td>\n      <td>1</td>\n      <td>[CWE-264]</td>\n      <td>php-src</td>\n      <td>055ecbc62878e86287d742c7246c21606cee8183</td>\n      <td>211824207069112513181516095447837228041</td>\n      <td>22</td>\n      <td>Improve check for :memory: pseudo-filename in ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>unpack_Z_stream(int fd_in, int fd_out)\\n{\\n\\tI...</td>\n      <td>1</td>\n      <td>[]</td>\n      <td>busybox</td>\n      <td>251fc70e9722f931eec23a34030d05ba5f747b0e</td>\n      <td>21401706257394042943815500829552774160</td>\n      <td>232</td>\n      <td>uncompress: fix buffer underrun by corrupted i...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>static void cirrus_do_copy(CirrusVGAState *s, ...</td>\n      <td>1</td>\n      <td>[CWE-787]</td>\n      <td>qemu</td>\n      <td>b2eb849d4b1fdb6f35d5c46958c7f703cf64cfef</td>\n      <td>135590882627853658533498335902319684573</td>\n      <td>66</td>\n      <td>CVE-2007-1320 - Cirrus LGD-54XX \"bitblt\" heap ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>glue(cirrus_bitblt_rop_fwd_, ROP_NAME)(CirrusV...</td>\n      <td>1</td>\n      <td>[CWE-787]</td>\n      <td>qemu</td>\n      <td>b2eb849d4b1fdb6f35d5c46958c7f703cf64cfef</td>\n      <td>27696392987383562433164405181263025184</td>\n      <td>18</td>\n      <td>CVE-2007-1320 - Cirrus LGD-54XX \"bitblt\" heap ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"# Cell 2 — Clean & Stratified Split (UPDATED)\nfrom sklearn.model_selection import train_test_split\n\n# basic cleaning\ndf = df.dropna(subset=[\"func\", \"target\"]).copy()\ndf[\"target\"] = df[\"target\"].astype(int)\n\n# features/labels\nX = df[\"func\"].astype(str)\ny = df[\"target\"].astype(int)\n\n# stratified 70 / 15 / 15\nX_train, X_tmp, y_train, y_tmp = train_test_split(\n    X, y, test_size=0.30, random_state=42, stratify=y\n)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_tmp, y_tmp, test_size=0.50, random_state=42, stratify=y_tmp\n)\n\nprint(\"Split sizes:\",\n      \"train =\", len(X_train),\n      \"| val =\", len(X_val),\n      \"| test =\", len(X_test))\nprint(\"Class ratio (train):\", y_train.mean())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T10:45:05.607429Z","iopub.execute_input":"2025-08-19T10:45:05.607780Z","iopub.status.idle":"2025-08-19T10:45:06.495643Z","shell.execute_reply.started":"2025-08-19T10:45:05.607761Z","shell.execute_reply":"2025-08-19T10:45:06.494981Z"}},"outputs":[{"name":"stdout","text":"Split sizes: train = 231344 | val = 49574 | test = 49574\nClass ratio (train): 0.05732156442354243\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch.multiprocessing as mp\nmp.set_start_method('spawn', force=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T10:45:06.496314Z","iopub.execute_input":"2025-08-19T10:45:06.496710Z","iopub.status.idle":"2025-08-19T10:45:10.653733Z","shell.execute_reply.started":"2025-08-19T10:45:06.496682Z","shell.execute_reply":"2025-08-19T10:45:10.653169Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Function to define StreamingGraphDataset and related initializations\ndef initialize_streaming_dataset():\n    import torch\n    from torch.utils.data import Dataset\n    from transformers import AutoTokenizer, AutoModel\n    import torch.nn.functional as F\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Initialize tokenizer and encoder\n    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n    encoder = AutoModel.from_pretrained(\"microsoft/codebert-base\").to(device)\n    encoder.eval()\n    for p in encoder.parameters(): \n        p.requires_grad_(False)\n\n    MAX_TOKENS = 128\n    WINDOW = 3\n    BATCH = 16\n\n    class StreamingGraphDataset(Dataset):\n        \"\"\"Streaming dataset that yields tokenized and graph-structured data on-the-fly.\"\"\"\n        def __init__(self, Xseries, yseries):\n            self.X = Xseries.astype(str).tolist()\n            self.y = yseries.tolist()\n\n        def __len__(self):\n            return len(self.X)\n\n        @torch.inference_mode()\n        def __getitem__(self, idx):\n            # Tokenize the text and obtain embeddings\n            enc = tokenizer(self.X[idx], padding=True, truncation=True, max_length=MAX_TOKENS, return_tensors=\"pt\")\n            enc = {k: v.to(device) for k, v in enc.items()}\n\n            last_hid = encoder(**enc).last_hidden_state  # [B, T, 768]\n            attn = enc[\"attention_mask\"].cpu()           # [B, T]\n            feats = last_hid.detach().cpu().to(torch.float16)  # [T, 768] fp16\n\n            tlen = int(attn.sum().item())  # Length after masking\n            x = feats[:tlen, :]  # Slice out actual tokens (no padding)\n            edge_index = self._build_window_edges(tlen, WINDOW)  # [2, E]\n            y = torch.tensor(self.y[idx], dtype=torch.long)  # Label\n\n            return x, edge_index, y\n\n        def _build_window_edges(self, tlen: int, window: int):\n            \"\"\"Sliding window graph edges (undirected with self-loops).\"\"\"\n            if tlen <= 1:\n                return torch.zeros(2, 0, dtype=torch.long)\n            src, dst = [], []\n            for i in range(tlen):\n                jmax = min(tlen, i + window)\n                for j in range(i + 1, jmax):\n                    src.append(i); dst.append(j)\n                    src.append(j); dst.append(i)\n            return torch.tensor([src, dst], dtype=torch.long)\n\n    return StreamingGraphDataset  # Return the class to be used later\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T10:52:19.932629Z","iopub.execute_input":"2025-08-19T10:52:19.932890Z","iopub.status.idle":"2025-08-19T10:52:19.942275Z","shell.execute_reply.started":"2025-08-19T10:52:19.932871Z","shell.execute_reply":"2025-08-19T10:52:19.941497Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Call the function to define StreamingGraphDataset\nStreamingGraphDataset = initialize_streaming_dataset()\n\n# Create datasets and dataloaders (no files saved to disk)\ntrain_set = StreamingGraphDataset(X_train, y_train)\nval_set = StreamingGraphDataset(X_val, y_val)\ntest_set = StreamingGraphDataset(X_test, y_test)\n\n# Using DataLoader to handle batching\ntrain_loader = DataLoader(train_set, batch_size=BATCH, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_set, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_set, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n\n# Check the dataset sizes\nlen(train_set), len(val_set), len(test_set)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T10:52:46.324733Z","iopub.execute_input":"2025-08-19T10:52:46.325018Z","iopub.status.idle":"2025-08-19T10:52:47.437986Z","shell.execute_reply.started":"2025-08-19T10:52:46.324987Z","shell.execute_reply":"2025-08-19T10:52:47.437256Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(231344, 49574, 49574)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4 — Deep Residual GCN (UPDATED)\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef add_self_loops(edge_index, num_nodes):\n    if edge_index.numel() == 0:\n        idx = torch.arange(num_nodes, device=edge_index.device)\n        return torch.stack([idx, idx], dim=0)\n    idx = torch.arange(num_nodes, device=edge_index.device)\n    self_loops = torch.stack([idx, idx], dim=0)\n    return torch.cat([edge_index, self_loops], dim=1)\n\ndef drop_edge(edge_index, p: float, training: bool):\n    if not training or p <= 0.0:\n        return edge_index\n    E = edge_index.size(1)\n    keep = torch.rand(E, device=edge_index.device) > p\n    return edge_index[:, keep]\n\nclass GCNConv(nn.Module):\n    \"\"\"Vanilla GCN conv with symmetric normalization.\"\"\"\n    def __init__(self, in_dim, out_dim, bias=True):\n        super().__init__()\n        self.lin = nn.Linear(in_dim, out_dim, bias=bias)\n    def forward(self, x, edge_index):\n        N = x.size(0)\n        edge_index = add_self_loops(edge_index, N)\n        src, dst = edge_index\n        deg = torch.bincount(dst, minlength=N).float().to(x.device).clamp_min_(1.0)\n        norm = deg.pow(-0.5)\n        x = self.lin(x)\n        out = torch.zeros_like(x)\n        msg = x[src] * norm[src].unsqueeze(1)\n        out.index_add_(0, dst, msg)\n        out = out * norm.unsqueeze(1)\n        return out\n\nclass AttnPool(nn.Module):\n    \"\"\"Attention pooling across nodes (batch_size=1 graphs).\"\"\"\n    def __init__(self, dim):\n        super().__init__()\n        self.proj  = nn.Linear(dim, dim)\n        self.score = nn.Linear(dim, 1)\n    def forward(self, x):\n        h = torch.tanh(self.proj(x))     # [N, F]\n        a = self.score(h).squeeze(-1)    # [N]\n        a = torch.softmax(a, dim=0)\n        return torch.sum(x * a.unsqueeze(-1), dim=0)  # [F]\n\nclass DeepGCNClassifier(nn.Module):\n    def __init__(\n        self,\n        in_dim=768,\n        hidden_dim=256,\n        num_layers=6,      # deeper but stable with residuals\n        dropout=0.30,\n        dropedge=0.10,\n        num_classes=2,\n        use_layernorm=True,\n        use_input_proj=True,\n        pooling=\"attn\",    # \"attn\" or \"mean\"\n    ):\n        super().__init__()\n        self.dropout   = dropout\n        self.dropedge  = dropedge\n        self.pooling   = pooling\n\n        self.input_proj = nn.Linear(in_dim, hidden_dim) if use_input_proj else nn.Identity()\n        c = hidden_dim if use_input_proj else in_dim\n\n        self.convs = nn.ModuleList([GCNConv(c, c) for _ in range(num_layers)])\n        self.norms = nn.ModuleList([nn.LayerNorm(c) if use_layernorm else nn.Identity() for _ in range(num_layers)])\n        self.pool  = AttnPool(c) if pooling == \"attn\" else None\n\n        self.head = nn.Sequential(\n            nn.LayerNorm(c),\n            nn.Linear(c, c),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(c, num_classes),\n        )\n\n    def forward(self, x, edge_index):\n        x = self.input_proj(x)\n        for conv, norm in zip(self.convs, self.norms):\n            res = x\n            x = norm(x)\n            x = F.relu(x, inplace=True)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            ei = drop_edge(edge_index, self.dropedge, self.training)\n            x = conv(x, ei)\n            if x.shape == res.shape:\n                x = x + res\n        g = self.pool(x) if self.pool is not None else x.mean(dim=0)\n        logits = self.head(g)   # [C]\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T10:52:54.472362Z","iopub.execute_input":"2025-08-19T10:52:54.472658Z","iopub.status.idle":"2025-08-19T10:52:54.516973Z","shell.execute_reply.started":"2025-08-19T10:52:54.472637Z","shell.execute_reply":"2025-08-19T10:52:54.516333Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Cell 5 — Training Loop & Evaluation (UPDATED)\nimport torch.optim as optim\n\ndef _unwrap_batch(item):\n    if isinstance(item, dict):\n        return item[\"x\"], item[\"edge_index\"], item[\"y\"]\n    if isinstance(item, (list, tuple)) and len(item) == 3:\n        x, ei, y = item\n        if isinstance(x, (list, tuple)): x = x[0]\n        if isinstance(ei, (list, tuple)): ei = ei[0]\n        if isinstance(y, (list, tuple)): y = y[0]\n        return x, ei, y\n    if isinstance(item, (list, tuple)) and len(item) == 1:\n        return _unwrap_batch(item[0])\n    raise ValueError(\"Unexpected batch format\")\n\ndef run_epoch(model, loader, optimizer=None):\n    train_mode = optimizer is not None\n    model.train(train_mode)\n    total_loss, total, correct = 0.0, 0, 0\n    for item in loader:\n        x, edge_index, y = _unwrap_batch(item)\n        x = x.to(device).float()\n        edge_index = edge_index.to(device).long()\n        y = y.to(device).long().view(-1)[0]  # scalar\n\n        logits = model(x, edge_index)                # [C]\n        loss = F.cross_entropy(logits.unsqueeze(0), y.unsqueeze(0))\n\n        if train_mode:\n            optimizer.zero_grad(set_to_none=True)\n            loss.backward()\n            optimizer.step()\n\n        total_loss += float(loss.item())\n        pred = int(logits.argmax().item())\n        correct += int(pred == int(y.item()))\n        total += 1\n\n    avg_loss = total_loss / max(total, 1)\n    acc = correct / max(total, 1)\n    return avg_loss, acc\n\n# Instantiate model/optimizer\ngcn = DeepGCNClassifier(\n    in_dim=768,\n    hidden_dim=256,\n    num_layers=6,\n    dropout=0.30,\n    dropedge=0.10,\n    num_classes=2,\n    pooling=\"attn\",\n).to(device)\n\noptimizer = optim.AdamW(gcn.parameters(), lr=1e-3, weight_decay=1e-4)\n\n# === EPOCHS: change this number if you want ===\nEPOCHS = 3\n\nbest_val = 0.0\nbest_path = \"/kaggle/working/best_gcn.pt\"\n\nfor ep in range(1, EPOCHS + 1):\n    tr_loss, tr_acc = run_epoch(gcn, train_loader, optimizer)\n    va_loss, va_acc = run_epoch(gcn, val_loader, None)\n    if va_acc > best_val:\n        best_val = va_acc\n        torch.save(gcn.state_dict(), best_path)\n    print(f\"epoch {ep}: train loss {tr_loss:.4f} acc {tr_acc:.3f} | val acc {va_acc:.3f} (best {best_val:.3f})\")\n\n# Test with best checkpoint\ngcn.load_state_dict(torch.load(best_path, map_location=device))\nte_loss, te_acc = run_epoch(gcn, test_loader, None)\nprint(f\"TEST: loss {te_loss:.4f} acc {te_acc:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T10:52:57.502032Z","iopub.execute_input":"2025-08-19T10:52:57.502307Z","iopub.status.idle":"2025-08-19T10:52:57.555838Z","shell.execute_reply.started":"2025-08-19T10:52:57.502284Z","shell.execute_reply":"2025-08-19T10:52:57.554877Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/767156583.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgcn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mva_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgcn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mva_acc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/767156583.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(model, loader, optimizer)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unwrap_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_spawn_posix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mset_spawning_popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mForkingPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'initialize_streaming_dataset.<locals>.StreamingGraphDataset'"],"ename":"AttributeError","evalue":"Can't pickle local object 'initialize_streaming_dataset.<locals>.StreamingGraphDataset'","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
