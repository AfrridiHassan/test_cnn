{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:14:47.055917Z",
     "iopub.status.busy": "2025-08-10T07:14:47.055409Z",
     "iopub.status.idle": "2025-08-10T07:14:54.125895Z",
     "shell.execute_reply": "2025-08-10T07:14:54.125169Z",
     "shell.execute_reply.started": "2025-08-10T07:14:47.055894Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330492, 8)\n",
      "Index(['func', 'target', 'cwe', 'project', 'commit_id', 'hash', 'size',\n",
      "       'message'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>func</th>\n",
       "      <th>target</th>\n",
       "      <th>cwe</th>\n",
       "      <th>project</th>\n",
       "      <th>commit_id</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>int _gnutls_ciphertext2compressed(gnutls_sessi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>gnutls</td>\n",
       "      <td>7ad6162573ba79a4392c63b453ad0220ca6c5ace</td>\n",
       "      <td>73008646937836648589283922871188272089</td>\n",
       "      <td>157</td>\n",
       "      <td>added an extra check while checking the padding.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>static char *make_filename_safe(const char *fi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[CWE-264]</td>\n",
       "      <td>php-src</td>\n",
       "      <td>055ecbc62878e86287d742c7246c21606cee8183</td>\n",
       "      <td>211824207069112513181516095447837228041</td>\n",
       "      <td>22</td>\n",
       "      <td>Improve check for :memory: pseudo-filename in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unpack_Z_stream(int fd_in, int fd_out)\\n{\\n\\tI...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>busybox</td>\n",
       "      <td>251fc70e9722f931eec23a34030d05ba5f747b0e</td>\n",
       "      <td>21401706257394042943815500829552774160</td>\n",
       "      <td>232</td>\n",
       "      <td>uncompress: fix buffer underrun by corrupted i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>static void cirrus_do_copy(CirrusVGAState *s, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[CWE-787]</td>\n",
       "      <td>qemu</td>\n",
       "      <td>b2eb849d4b1fdb6f35d5c46958c7f703cf64cfef</td>\n",
       "      <td>135590882627853658533498335902319684573</td>\n",
       "      <td>66</td>\n",
       "      <td>CVE-2007-1320 - Cirrus LGD-54XX \"bitblt\" heap ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>glue(cirrus_bitblt_rop_fwd_, ROP_NAME)(CirrusV...</td>\n",
       "      <td>1</td>\n",
       "      <td>[CWE-787]</td>\n",
       "      <td>qemu</td>\n",
       "      <td>b2eb849d4b1fdb6f35d5c46958c7f703cf64cfef</td>\n",
       "      <td>27696392987383562433164405181263025184</td>\n",
       "      <td>18</td>\n",
       "      <td>CVE-2007-1320 - Cirrus LGD-54XX \"bitblt\" heap ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                func  target        cwe  \\\n",
       "0  int _gnutls_ciphertext2compressed(gnutls_sessi...       1         []   \n",
       "1  static char *make_filename_safe(const char *fi...       1  [CWE-264]   \n",
       "2  unpack_Z_stream(int fd_in, int fd_out)\\n{\\n\\tI...       1         []   \n",
       "3  static void cirrus_do_copy(CirrusVGAState *s, ...       1  [CWE-787]   \n",
       "4  glue(cirrus_bitblt_rop_fwd_, ROP_NAME)(CirrusV...       1  [CWE-787]   \n",
       "\n",
       "   project                                 commit_id  \\\n",
       "0   gnutls  7ad6162573ba79a4392c63b453ad0220ca6c5ace   \n",
       "1  php-src  055ecbc62878e86287d742c7246c21606cee8183   \n",
       "2  busybox  251fc70e9722f931eec23a34030d05ba5f747b0e   \n",
       "3     qemu  b2eb849d4b1fdb6f35d5c46958c7f703cf64cfef   \n",
       "4     qemu  b2eb849d4b1fdb6f35d5c46958c7f703cf64cfef   \n",
       "\n",
       "                                      hash  size  \\\n",
       "0   73008646937836648589283922871188272089   157   \n",
       "1  211824207069112513181516095447837228041    22   \n",
       "2   21401706257394042943815500829552774160   232   \n",
       "3  135590882627853658533498335902319684573    66   \n",
       "4   27696392987383562433164405181263025184    18   \n",
       "\n",
       "                                             message  \n",
       "0   added an extra check while checking the padding.  \n",
       "1  Improve check for :memory: pseudo-filename in ...  \n",
       "2  uncompress: fix buffer underrun by corrupted i...  \n",
       "3  CVE-2007-1320 - Cirrus LGD-54XX \"bitblt\" heap ...  \n",
       "4  CVE-2007-1320 - Cirrus LGD-54XX \"bitblt\" heap ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"/kaggle/input/diversevul-20230702/diversevul_20230702.json\"\n",
    "\n",
    "import json\n",
    "\n",
    "rows = []\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:24:05.549578Z",
     "iopub.status.busy": "2025-08-10T07:24:05.549342Z",
     "iopub.status.idle": "2025-08-10T07:24:06.685065Z",
     "shell.execute_reply": "2025-08-10T07:24:06.684495Z",
     "shell.execute_reply.started": "2025-08-10T07:24:05.549562Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231344, 49574, 49574, 0.05732156442354243)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# assumes df already exists from your load step\n",
    "df = df.dropna(subset=[\"func\",\"target\"]).copy()\n",
    "df[\"target\"] = df[\"target\"].astype(int)\n",
    "\n",
    "X = df[\"func\"].astype(str)\n",
    "y = df[\"target\"].astype(int)\n",
    "\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.50, random_state=42, stratify=y_tmp)\n",
    "\n",
    "len(X_train), len(X_val), len(X_test), y_train.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:24:55.007913Z",
     "iopub.status.busy": "2025-08-10T07:24:55.007226Z",
     "iopub.status.idle": "2025-08-10T07:25:42.831613Z",
     "shell.execute_reply": "2025-08-10T07:25:42.830773Z",
     "shell.execute_reply.started": "2025-08-10T07:24:55.007877Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c4203810894f3d92fe42fbcea993ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a11d012ec242d89ced19f55c4a1441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46b93f24db74ad8857a3a6d26649696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faebc34892244c849dbe180e2ab5eb59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01b4e4d39f04098a693bd3ad942df68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 07:25:23.170570: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754810723.519428      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754810723.619383      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534e727a9e2c4fc5ba03bdf0570fcd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9e04af58de4dc7b255fac0e08e43e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install transformers if needed\n",
    "try:\n",
    "    import transformers  # noqa\n",
    "except Exception:\n",
    "    !pip -q install transformers\n",
    "\n",
    "import torch, os, math\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\").to(device)\n",
    "model.eval();  # freeze\n",
    "for p in model.parameters(): p.requires_grad = False\n",
    "\n",
    "MAX_TOKENS = 128     # keep small; raise to 256 later if you want\n",
    "WINDOW = 3           # sliding window size\n",
    "BATCH = 16           # adjust based on VRAM/RAM\n",
    "\n",
    "os.makedirs(\"/kaggle/working/graphs/train\", exist_ok=True)\n",
    "os.makedirs(\"/kaggle/working/graphs/val\", exist_ok=True)\n",
    "os.makedirs(\"/kaggle/working/graphs/test\", exist_ok=True)\n",
    "\n",
    "def build_edges(n_tokens:int, win:int=WINDOW):\n",
    "    # undirected edges within [i, i+1..i+win-1]\n",
    "    src, dst = [], []\n",
    "    for i in range(n_tokens):\n",
    "        for j in range(i+1, min(n_tokens, i+win)):\n",
    "            src += [i, j]; dst += [j, i]\n",
    "    if not src:\n",
    "        return torch.empty(2,0, dtype=torch.long)\n",
    "    return torch.tensor([src, dst], dtype=torch.long)\n",
    "\n",
    "def save_graph_batch(texts, labels, out_dir, start_idx):\n",
    "    enc = tokenizer(texts, padding=True, truncation=True, max_length=MAX_TOKENS, return_tensors=\"pt\")\n",
    "    enc = {k:v.to(device) for k,v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model(**enc).last_hidden_state  # [B, T, 768]\n",
    "    attn = enc[\"attention_mask\"].cpu()       # [B, T]\n",
    "\n",
    "    for i in range(out.size(0)):\n",
    "        tlen = int(attn[i].sum().item())\n",
    "        x = out[i, :tlen].half().cpu()       # [tlen, 768] fp16 to save space\n",
    "        edge_index = build_edges(tlen)       # [2, E]\n",
    "        y = torch.tensor([int(labels[i])], dtype=torch.long)\n",
    "        idx = start_idx + i\n",
    "        torch.save({\"x\": x, \"edge_index\": edge_index, \"y\": y},\n",
    "                   os.path.join(out_dir, f\"g_{idx}.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:26:41.440718Z",
     "iopub.status.busy": "2025-08-10T07:26:41.439433Z",
     "iopub.status.idle": "2025-08-10T07:28:29.744055Z",
     "shell.execute_reply": "2025-08-10T07:28:29.743324Z",
     "shell.execute_reply.started": "2025-08-10T07:26:41.440680Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [01:17<00:00,  8.11it/s]\n",
      "100%|██████████| 125/125 [00:15<00:00,  7.99it/s]\n",
      "100%|██████████| 125/125 [00:15<00:00,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train files: 10000\n",
      "val files: 2000\n",
      "test files: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "from tqdm import trange\n",
    "\n",
    "# how many samples to process now\n",
    "N_TRAIN, N_VAL, N_TEST = 10_000, 2_000, 2_000\n",
    "\n",
    "def process_split(Xseries, yseries, out_dir, limit):\n",
    "    texts = Xseries.iloc[:limit].tolist()\n",
    "    labels = yseries.iloc[:limit].tolist()\n",
    "    total = len(texts)\n",
    "    for s in trange(0, total, BATCH):\n",
    "        e = min(s+BATCH, total)\n",
    "        save_graph_batch(texts[s:e], labels[s:e], out_dir, start_idx=s)\n",
    "\n",
    "process_split(X_train, y_train, \"/kaggle/working/graphs/train\", N_TRAIN)\n",
    "process_split(X_val,   y_val,   \"/kaggle/working/graphs/val\",   N_VAL)\n",
    "process_split(X_test,  y_test,  \"/kaggle/working/graphs/test\",  N_TEST)\n",
    "\n",
    "# quick sanity: how many files did we write?\n",
    "print(\"train files:\", len(os.listdir(\"/kaggle/working/graphs/train\")))\n",
    "print(\"val files:\",   len(os.listdir(\"/kaggle/working/graphs/val\")))\n",
    "print(\"test files:\",  len(os.listdir(\"/kaggle/working/graphs/test\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:29:41.338322Z",
     "iopub.status.busy": "2025-08-10T07:29:41.337928Z",
     "iopub.status.idle": "2025-08-10T07:29:41.349873Z",
     "shell.execute_reply": "2025-08-10T07:29:41.349340Z",
     "shell.execute_reply.started": "2025-08-10T07:29:41.338299Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/graphs/train/g_2773.pt\n",
      "x: torch.Size([54, 768]) torch.float16\n",
      "edge_index: torch.Size([2, 210])\n",
      "y: 0\n"
     ]
    }
   ],
   "source": [
    "import torch, os, random\n",
    "\n",
    "sample_path = os.path.join(\"/kaggle/working/graphs/train\", random.choice(os.listdir(\"/kaggle/working/graphs/train\")))\n",
    "g = torch.load(sample_path)\n",
    "print(sample_path)\n",
    "print(\"x:\", g[\"x\"].shape, g[\"x\"].dtype)       # [num_nodes, 768] float16\n",
    "print(\"edge_index:\", g[\"edge_index\"].shape)   # [2, num_edges]\n",
    "print(\"y:\", g[\"y\"].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:42:17.377291Z",
     "iopub.status.busy": "2025-08-10T07:42:17.376981Z",
     "iopub.status.idle": "2025-08-10T07:42:17.416724Z",
     "shell.execute_reply": "2025-08-10T07:42:17.416231Z",
     "shell.execute_reply.started": "2025-08-10T07:42:17.377271Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, glob, math, random, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---- dataset: loads your saved .pt graphs (x, edge_index, y) ----\n",
    "class GraphDir(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, limit=None, shuffle=True):\n",
    "        self.paths = sorted(glob.glob(os.path.join(root, \"g_*.pt\")))\n",
    "        if shuffle:\n",
    "            random.shuffle(self.paths)\n",
    "        if limit is not None:\n",
    "            self.paths = self.paths[:limit]\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        g = torch.load(self.paths[idx])\n",
    "        return g[\"x\"].float(), g[\"edge_index\"].long(), g[\"y\"].long()\n",
    "\n",
    "train_ds = GraphDir(\"/kaggle/working/graphs/train\")   # optionally pass limit=2000 for speed\n",
    "val_ds   = GraphDir(\"/kaggle/working/graphs/val\")\n",
    "test_ds  = GraphDir(\"/kaggle/working/graphs/test\")\n",
    "\n",
    "# simple batcher: process graphs one-by-one to keep it robust/lightweight\n",
    "def iterate(dl):\n",
    "    for item in dl:\n",
    "        yield item\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=1, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "test_loader  = torch.utils.data.DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T08:17:01.762195Z",
     "iopub.status.busy": "2025-08-10T08:17:01.761561Z",
     "iopub.status.idle": "2025-08-10T08:19:25.125632Z",
     "shell.execute_reply": "2025-08-10T08:19:25.124940Z",
     "shell.execute_reply.started": "2025-08-10T08:17:01.762173Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train loss 0.2686, acc 0.943 | val acc 0.940\n",
      "epoch 2: train loss 0.2789, acc 0.944 | val acc 0.940\n",
      "epoch 3: train loss 0.2733, acc 0.944 | val acc 0.940\n",
      "test acc 0.941\n"
     ]
    }
   ],
   "source": [
    "# --- GCN (pure torch) ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def add_self_loops(edge_index, num_nodes):\n",
    "    if edge_index.numel() == 0:\n",
    "        return torch.stack([torch.arange(num_nodes), torch.arange(num_nodes)], dim=0)\n",
    "    self_loops = torch.arange(num_nodes, device=edge_index.device)\n",
    "    self_loops = torch.stack([self_loops, self_loops], dim=0)\n",
    "    return torch.cat([edge_index, self_loops], dim=1)\n",
    "\n",
    "class GCNConv(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, bias=True):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(in_dim, out_dim, bias=bias)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # edge_index: [2, E] with src->dst\n",
    "        N = x.size(0)\n",
    "        edge_index = add_self_loops(edge_index, N)\n",
    "        src, dst = edge_index\n",
    "\n",
    "        # degree (with self-loops)\n",
    "        deg = torch.bincount(dst, minlength=N).float().to(x.device)\n",
    "\n",
    "        # normalized message passing: D^-1/2 A D^-1/2 X\n",
    "        norm = 1.0 / torch.sqrt(torch.clamp(deg, min=1.0))\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # aggregate\n",
    "        out = torch.zeros_like(x)\n",
    "        msg = x[src] * norm[src].unsqueeze(1)\n",
    "        out.index_add_(0, dst, msg)\n",
    "        out = out * norm.unsqueeze(1)\n",
    "        return out\n",
    "\n",
    "class GCNClassifier(nn.Module):\n",
    "    def __init__(self, in_dim=768, hid=256, out_dim=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hid)\n",
    "        self.conv2 = GCNConv(hid, hid)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.cls = nn.Linear(hid, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index); x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index); x = F.relu(x)\n",
    "        g = x.mean(dim=0, keepdim=True)        # global mean pool\n",
    "        g = self.dropout(g)\n",
    "        return self.cls(g).squeeze(0)\n",
    "\n",
    "# --- train/eval (same as before) ---\n",
    "def run_epoch(model, loader, optim=None):\n",
    "    is_train = optim is not None\n",
    "    model.train(is_train)\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, edge_index, y in loader:\n",
    "        x, edge_index, y = x[0].to(device), edge_index[0].to(device), y[0].to(device)\n",
    "        if is_train: optim.zero_grad()\n",
    "        logits = model(x, edge_index)\n",
    "        loss = F.cross_entropy(logits.unsqueeze(0), y)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        total_loss += loss.item()\n",
    "        correct += int(logits.argmax().item() == int(y.item()))\n",
    "        total += 1\n",
    "    return total_loss/max(total,1), correct/max(total,1)\n",
    "\n",
    "model = GCNClassifier(in_dim=768, hid=256, out_dim=2, dropout=0.2).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "EPOCHS = 3\n",
    "best_val = 0.0\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc = run_epoch(model, train_loader, opt)\n",
    "    va_loss, va_acc = run_epoch(model, val_loader, None)\n",
    "    if va_acc > best_val:\n",
    "        best_val = va_acc\n",
    "        torch.save(model.state_dict(), \"/kaggle/working/best_gcn.pt\")\n",
    "    print(f\"epoch {ep}: train loss {tr_loss:.4f}, acc {tr_acc:.3f} | val acc {va_acc:.3f}\")\n",
    "\n",
    "# test\n",
    "model.load_state_dict(torch.load(\"/kaggle/working/best_gcn.pt\", map_location=device))\n",
    "te_loss, te_acc = run_epoch(model, test_loader, None)\n",
    "print(f\"test acc {te_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8040081,
     "sourceId": 12720504,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
